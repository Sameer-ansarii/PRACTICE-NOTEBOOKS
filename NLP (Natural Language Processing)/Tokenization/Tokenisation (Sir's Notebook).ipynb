{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation\n",
    "\n",
    "The notebook contains three types of tokenisation techniques:\n",
    "1. Word tokenisation\n",
    "2. Sentence tokenisation\n",
    "3. Tweet tokenisation\n",
    "4. Custom tokenisation using regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Word tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At nine o'clock I visited him myself. It looks like religious mania, and he'll soon think that he himself is God.\n"
     ]
    }
   ],
   "source": [
    "document = \"At nine o'clock I visited him myself. It looks like religious mania, and he'll soon think that he himself is God.\"\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenising on spaces using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'nine', \"o'clock\", 'I', 'visited', 'him', 'myself.', 'It', 'looks', 'like', 'religious', 'mania,', 'and', \"he'll\", 'soon', 'think', 'that', 'he', 'himself', 'is', 'God.']\n"
     ]
    }
   ],
   "source": [
    "print(document.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenising using nltk word tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'nine', \"o'clock\", 'I', 'visited', 'him', 'myself', '.', 'It', 'looks', 'like', 'religious', 'mania', ',', 'and', 'he', \"'ll\", 'soon', 'think', 'that', 'he', 'himself', 'is', 'God', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's word tokeniser not only breaks on whitespaces but also breaks contraction words such as he'll into \"he\" and \"'ll\". On the other hand it doesn't break \"o'clock\" and treats it as a separate token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sentence tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenising based on sentence requires you to split on the period ('.'). Let's use nltk sentence tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"At nine o'clock I visited him myself.\", \"It looks like religious mania, and he'll soon think that he himself is God.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tweet tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with word tokeniser is that it fails to tokeniser emojis and other complex special characters such as word with hashtags. Emojis are common these days and people use them all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"i recently watched this show called mindhunters:). i totally loved it üòç. it was gr8 <3. #bingewatching #nothingtodo üòé\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'recently', 'watched', 'this', 'show', 'called', 'mindhunters', ':', ')', '.', 'i', 'totally', 'loved', 'it', 'üòç', '.', 'it', 'was', 'gr8', '<', '3', '.', '#', 'bingewatching', '#', 'nothingtodo', 'üòé']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word tokeniser breaks the emoji '<3' into '<' and '3' which is something that we don't want. Emojis have their own significance in areas like sentiment analysis where a happy face and sad face can salone prove to be a really good predictor of the sentiment. Similarly, the hashtags are broken into two tokens. A hashtag is used for searching specific topics or photos in social media apps such as Instagram and facebook. So there, you want to use the hashtag as is.\n",
    "\n",
    "Let's use the tweet tokeniser of nltk to tokenise this message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'recently',\n",
       " 'watched',\n",
       " 'this',\n",
       " 'show',\n",
       " 'called',\n",
       " 'mindhunters',\n",
       " ':)',\n",
       " '.',\n",
       " 'i',\n",
       " 'totally',\n",
       " 'loved',\n",
       " 'it',\n",
       " 'üòç',\n",
       " '.',\n",
       " 'it',\n",
       " 'was',\n",
       " 'gr8',\n",
       " '<3',\n",
       " '.',\n",
       " '#bingewatching',\n",
       " '#nothingtodo',\n",
       " 'üòé']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr.tokenize(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it handles all the emojis and the hashtags pretty well.\n",
    "\n",
    "Now, there is a tokeniser that takes a regular expression and tokenises and returns result based on the pattern of regular expression.\n",
    "\n",
    "Let's look at how you can use regular expression tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "message = \"i recently watched this show called mindhunters:). i totally loved it üòç. it was gr8 <3. #bingewatching #nothingtodo üòé\"\n",
    "pattern = \"#[\\w]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#bingewatching', '#nothingtodo']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(message, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Write a piece of code that breaks a given sentence into words and store them in a list. Then print the list as well as the length of the list. Use the NLTK tokeniser to tokenise words.\n",
    "\n",
    "Sample input:\n",
    "\n",
    "\"I love pasta\"\n",
    "\n",
    "Expected output:\n",
    "\n",
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love pasta\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import ast, sys\n",
    "sentence = input()\n",
    "\n",
    "# tokenise sentence into words\n",
    "words =word_tokenize(sentence) # write your code here\n",
    "\n",
    "# print length - don't change the following piece of code\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Write a piece of code that breaks a given sentence into words and stores them in a list. Then remove the stop words from this list and then print the list as well as the length of the list. Again, use the NLTK tokeniser to do this.\n",
    "\n",
    "Sample input: \n",
    "\n",
    "‚ÄúEducation is the most powerful weapon that you can use to change the world‚Äù\n",
    "\n",
    "Expected output: \n",
    "\n",
    "6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Education is the most powerful weapon that you can use to change the world\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import ast, sys\n",
    "sentence = input()\n",
    "\n",
    "# change sentence to lowercase\n",
    "sentence = sentence.lower() # write code here\n",
    "\n",
    "\n",
    "# tokenise sentence into words\n",
    "words = word_tokenize(sentence)# write code here\n",
    "\n",
    "# extract nltk stop word list\n",
    "stopwords = stopwords.words('english') # write code here\n",
    "\n",
    "# remove stop words\n",
    "no_stops = [x for x in words if x not in stopwords] # write code here\n",
    "\n",
    "# print length - don't change the following piece of code\n",
    "print(len(no_stops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Write a Python code using the NLTK library that breaks a given piece of text containing multiple sentences into different sentences. Finally print the total number of sentences in the text.\n",
    "\n",
    "Sample input: \n",
    "Develop a passion for your learning. If you do, you‚Äôll never cease to grow.\n",
    "\n",
    "Expected output:\n",
    "\n",
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Develop a passion for your learning. If you do, you‚Äôll never cease to grow.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import ast, sys\n",
    "text = input()\n",
    "\n",
    "# change sentence to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "\n",
    "# tokenise sentence into words\n",
    "sentences = sent_tokenize(text)# write code here\n",
    "\n",
    "# print length - don't change the following piece of code\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Use NLTK‚Äôs regex tokeniser to extract all the mentions from a given tweet and then print the total number of mentions. A mention comprises of a ‚Äò@‚Äô symbol followed by a username containing either alphabets, numbers or underscores.\n",
    "\n",
    "Sample tweet:\n",
    "So excited to be a part of machine learning and artificial intelligence program made by @upgrad and @iiitb\n",
    "\n",
    "Expected output:\n",
    "2 (because there are two mentions - ‚Äò@upgrad‚Äô and ‚Äò@iiitb‚Äô )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So excited to be a part of machine learning and artificial intelligence program made by @upgrad and @iiitb\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import ast, sys\n",
    "text = input()\n",
    "\n",
    "# change text to lowercase\n",
    "text = text.lower() # write code here\n",
    "\n",
    "# pattern to extract mentions\n",
    "pattern = \"@[\\w]+\"# write regex pattern here\n",
    "\n",
    "# extract mentions by using regex tokeniser\n",
    "mentions = regexp_tokenize(text, pattern)# write code here\n",
    "\n",
    "# print length - don't change the following piece of code\n",
    "print(len(mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
