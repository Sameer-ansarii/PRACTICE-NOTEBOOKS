{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557633b0",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "* Tokenization is a process of splitting or breaking down a text into individual units, called tokens. These tokens are typically words, but they can also be subwords, sentences(phrases) or individual characters. Tokenization is a important NLP tasks  because it enables the computer to understand the structure and meaning of a text more easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9c94d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll recently watched o'clock this show's called mindhunters:). \n",
      "I totally loved it üòç. It was gr8 <3! #bingewatching #nothingtodo üòé\n"
     ]
    }
   ],
   "source": [
    "corpus = \"\"\"I'll recently watched o'clock this show's called mindhunters:). \n",
    "I totally loved it üòç. It was gr8 <3! #bingewatching #nothingtodo üòé\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fad0b0",
   "metadata": {},
   "source": [
    "### Tokenising on spaces using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e510079d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'll\", 'recently', 'watched', \"o'clock\", 'this', \"show's\", 'called', 'mindhunters:).', 'I', 'totally', 'loved', 'it', 'üòç.', 'It', 'was', 'gr8', '<3!', '#bingewatching', '#nothingtodo', 'üòé']\n"
     ]
    }
   ],
   "source": [
    "print(corpus.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e31823",
   "metadata": {},
   "source": [
    "# Types of Tokenization in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f042d4c",
   "metadata": {},
   "source": [
    "## 1) Word Tokenizer\n",
    "\n",
    "\n",
    "* It splits or seprates the text into individual words,called tokens. It also seprates punctuation marks like .,!?, etc. and other characters also like #(hashtags) and emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e0c7492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'ll\",\n",
       " 'recently',\n",
       " 'watched',\n",
       " \"o'clock\",\n",
       " 'this',\n",
       " 'show',\n",
       " \"'s\",\n",
       " 'called',\n",
       " 'mindhunters',\n",
       " ':',\n",
       " ')',\n",
       " '.',\n",
       " 'I',\n",
       " 'totally',\n",
       " 'loved',\n",
       " 'it',\n",
       " 'üòç',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'gr8',\n",
       " '<',\n",
       " '3',\n",
       " '!',\n",
       " '#',\n",
       " 'bingewatching',\n",
       " '#',\n",
       " 'nothingtodo',\n",
       " 'üòé']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk stands for Natural Language Toolkit\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66d7c758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'ll\", 'recently', 'watched', \"o'clock\", 'this', 'show', \"'s\", 'called', 'mindhunters', ':', ')', '.', 'i', 'totally', 'loved', 'it', 'üòç', '.', 'it', 'was', 'gr8', '<', '3', '!', '#', 'bingewatching', '#', 'nothingtodo', 'üòé']\n"
     ]
    }
   ],
   "source": [
    "word = word_tokenize(corpus.lower(),language='English')\n",
    "\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1db3cae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'LL\", 'RECENTLY', 'WATCHED', \"O'CLOCK\", 'THIS', 'SHOW', \"'S\", 'CALLED', 'MINDHUNTERS', ':', ')', '.', 'I', 'TOTALLY', 'LOVED', 'IT', 'üòç', '.', 'IT', 'WAS', 'GR8', '<', '3', '!', '#', 'BINGEWATCHING', '#', 'NOTHINGTODO', 'üòé']\n"
     ]
    }
   ],
   "source": [
    "word = word_tokenize(corpus.upper())\n",
    "\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a4636",
   "metadata": {},
   "source": [
    "### Note:- \n",
    "\n",
    "* nltk's word tokenizer  breaks on whitespaces as well as it also breaks punctuation words such as \"I'll\" into \"I\" and \"'ll\", show's\" into \"show\" and \"'s\". On the other hand it doesn't break \"o'clock\" and treats it as a separate token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc42e3c9",
   "metadata": {},
   "source": [
    "## 2) Word Punctuation Tokenizer\n",
    "\n",
    "* It seprate text into individual words and also seprates all punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c4a5c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'\",\n",
       " 'll',\n",
       " 'recently',\n",
       " 'watched',\n",
       " 'o',\n",
       " \"'\",\n",
       " 'clock',\n",
       " 'this',\n",
       " 'show',\n",
       " \"'\",\n",
       " 's',\n",
       " 'called',\n",
       " 'mindhunters',\n",
       " ':).',\n",
       " 'I',\n",
       " 'totally',\n",
       " 'loved',\n",
       " 'it',\n",
       " 'üòç.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'gr8',\n",
       " '<',\n",
       " '3',\n",
       " '!',\n",
       " '#',\n",
       " 'bingewatching',\n",
       " '#',\n",
       " 'nothingtodo',\n",
       " 'üòé']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0a80d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'\", 'll', 'recently', 'watched', 'o', \"'\", 'clock', 'this', 'show', \"'\", 's', 'called', 'mindhunters', ':).', 'I', 'totally', 'loved', 'it', 'üòç.', 'It', 'was', 'gr8', '<', '3', '!', '#', 'bingewatching', '#', 'nothingtodo', 'üòé']\n"
     ]
    }
   ],
   "source": [
    "word_punctuation = wordpunct_tokenize(corpus)\n",
    "\n",
    "print(word_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a9fd81",
   "metadata": {},
   "source": [
    "## 3) Sentence Tokenizer\n",
    "\n",
    "\n",
    "* It splits or seprate the text into sentences. It Seprates sentences by .(fullstop) and !(Exclamation mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc00ef5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'll recently watched o'clock this show's called mindhunters:).\",\n",
       " 'I totally loved it üòç.',\n",
       " 'It was gr8 <3!',\n",
       " '#bingewatching #nothingtodo üòé']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df94f40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'll recently watched o'clock this show's called mindhunters:).\", 'I totally loved it üòç.', 'It was gr8 <3!', '#bingewatching #nothingtodo üòé'] \n",
      "\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "sentence = sent_tokenize(corpus)\n",
    "\n",
    "print(sentence,'\\n')\n",
    "\n",
    "# Print the length of senetences that are genrated after applying sent_tokenize\n",
    "print(len(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d05c5",
   "metadata": {},
   "source": [
    "## 4) Tweet Tokenizer \n",
    "\n",
    "* Word tokenizer or punctuation word tokenizer it will seprate text emojis like \"<3\" into '<' and '3' and \":)\" into ':' and ')' which is something that we don't want.\n",
    "\n",
    "\n",
    "* Emojis have their own significance in areas like sentiment analysis where a happy face and sad face can alone prove to be a really good predictor of the sentiment.\n",
    "\n",
    "\n",
    "* Similarly, the hashtags are broken into two tokens. A hashtag is used for searching specific topics or photos in social media apps such as Instagram and facebook.\n",
    "\n",
    "\n",
    "* Tweet tokenizer breakdown text into individual tokens except text emojis, #(hashtags) and (') apostrophe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c001e876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'll\",\n",
       " 'recently',\n",
       " 'watched',\n",
       " \"o'clock\",\n",
       " 'this',\n",
       " \"show's\",\n",
       " 'called',\n",
       " 'mindhunters',\n",
       " ':)',\n",
       " '.',\n",
       " 'I',\n",
       " 'totally',\n",
       " 'loved',\n",
       " 'it',\n",
       " 'üòç',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'gr8',\n",
       " '<3',\n",
       " '!',\n",
       " '#bingewatching',\n",
       " '#nothingtodo',\n",
       " 'üòé']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "TweetTokenizer().tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddf75293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'll\",\n",
       " 'recently',\n",
       " 'watched',\n",
       " \"o'clock\",\n",
       " 'this',\n",
       " \"show's\",\n",
       " 'called',\n",
       " 'mindhunters',\n",
       " ':)',\n",
       " '.',\n",
       " 'I',\n",
       " 'totally',\n",
       " 'loved',\n",
       " 'it',\n",
       " 'üòç',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'gr8',\n",
       " '<3',\n",
       " '!',\n",
       " '#bingewatching',\n",
       " '#nothingtodo',\n",
       " 'üòé']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c84998e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'll\", 'recently', 'watched', \"o'clock\", 'this', \"show's\", 'called', 'mindhunters', ':)', '.', 'I', 'totally', 'loved', 'it', 'üòç', '.', 'It', 'was', 'gr8', '<3', '!', '#bingewatching', '#nothingtodo', 'üòé']\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.tokenize(corpus)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843936a1",
   "metadata": {},
   "source": [
    "## 5) Regular Expression Tokenizer\n",
    "\n",
    "* Regex Tokenizer breakdown and output only that text which matches with regex pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9f9ad79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#bingewatching', '#nothingtodo']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "text = corpus\n",
    "\n",
    "pattern = \"#\\w+\"\n",
    "\n",
    "regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e3c81",
   "metadata": {},
   "source": [
    "## 6) Tree Bank Word Tokenizer\n",
    "\n",
    "\n",
    "* It breakdown text into tokens whether it is emoji, hastag or punctuation marks. \n",
    "\n",
    "\n",
    "* It will not breakdown fullstop(.)\n",
    "\n",
    "\n",
    "* It will breakdown fullstop when there is a space before fullstop or the last fullstop of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "357a5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1c4c8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'My Name is Sameer.live in #Nepal. Hel?lo . jd.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7600c3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'Name',\n",
       " 'is',\n",
       " 'Sameer.live',\n",
       " 'in',\n",
       " '#',\n",
       " 'Nepal.',\n",
       " 'Hel',\n",
       " '?',\n",
       " 'lo',\n",
       " '.',\n",
       " 'jd',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57f5f14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'ll\",\n",
       " 'recently',\n",
       " 'watched',\n",
       " \"o'clock\",\n",
       " 'this',\n",
       " 'show',\n",
       " \"'s\",\n",
       " 'called',\n",
       " 'mindhunters',\n",
       " ':',\n",
       " ')',\n",
       " '.',\n",
       " 'I',\n",
       " 'totally',\n",
       " 'loved',\n",
       " 'it',\n",
       " 'üòç.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'gr8',\n",
       " '<',\n",
       " '3',\n",
       " '!',\n",
       " '#',\n",
       " 'bingewatching',\n",
       " '#',\n",
       " 'nothingtodo',\n",
       " 'üòé']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b52366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'Name', 'is', 'Sameer.live', 'in', '#', 'Nepal.', 'Hel', '?', 'lo', '.', 'jd', '.']\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.tokenize(text)\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f9859c",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "\n",
    "* Stop words are common words that are often removed from a text before analysis because they don't add much value in terms of meaning. Examples of stop words include \"the\", \"and\", \"a\", and \"an\".\n",
    "\n",
    "\n",
    "* Removing stop words can help to improve the efficiency and accuracy of NLP tasks, such as sentiment analysis or topic modeling, because it reduces noise and helps to focus on the more important words in the text.\n",
    "\n",
    "\n",
    "* However, the set of stopwords used can vary depending on the task and language being analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5d87856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Extract all stopwords of english\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8826b94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'sentence', 'demonstrate', 'stop', 'word', 'removal', '.']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Use NLTK's built-in stop words corpus to remove stop words from a piece of text:-\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Text to be processed\n",
    "text = \"This is a sample sentence to demonstrate stop word removal.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Remove stop words\n",
    "filtered_tokens = [word for word in tokens if not word.lower() in stop_words]\n",
    "\n",
    "# Print the filtered tokens\n",
    "print(filtered_tokens)\n",
    "\n",
    "# Print the length of filtered_tokens\n",
    "print(len(filtered_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e31e1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
