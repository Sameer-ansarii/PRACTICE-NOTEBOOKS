{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557633b0",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "* Tokenization is a process of splitting or breaking down a text into individual units, called tokens. These tokens are typically words, but they can also be subwords, sentences(phrases) or individual characters. Tokenization is a important NLP tasks  because it enables the computer to understand the structure and meaning of a text more easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b9c94d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll recently watched o'clock this show's called mindhunters:). \n",
      "I totally loved it üòç. It was gr8 <3! #bingewatching #nothingtodo üòé\n"
     ]
    }
   ],
   "source": [
    "corpus = \"\"\"I'll recently watched o'clock this show's called mindhunters:). \n",
    "I totally loved it üòç. It was gr8 <3! #bingewatching #nothingtodo üòé\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fad0b0",
   "metadata": {},
   "source": [
    "### Tokenising on spaces using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e510079d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'll\", 'recently', 'watched', \"o'clock\", 'this', \"show's\", 'called', 'mindhunters:).', 'I', 'totally', 'loved', 'it', 'üòç.', 'It', 'was', 'gr8', '<3!', '#bingewatching', '#nothingtodo', 'üòé']\n"
     ]
    }
   ],
   "source": [
    "print(corpus.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e31823",
   "metadata": {},
   "source": [
    "# Types of Tokenization in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f042d4c",
   "metadata": {},
   "source": [
    "## 1) Word Tokenizer\n",
    "\n",
    "\n",
    "* It splits or seprates the text into individual words,called tokens. It also seprates punctuation marks like .,!?, etc. and other characters also like #(hashtags) and emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e0c7492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'ll\",\n",
       " 'recently',\n",
       " 'watched',\n",
       " \"o'clock\",\n",
       " 'this',\n",
       " 'show',\n",
       " \"'s\",\n",
       " 'called',\n",
       " 'mindhunters',\n",
       " ':',\n",
       " ')',\n",
       " '.',\n",
       " 'I',\n",
       " 'totally',\n",
       " 'loved',\n",
       " 'it',\n",
       " 'üòç',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'gr8',\n",
       " '<',\n",
       " '3',\n",
       " '!',\n",
       " '#',\n",
       " 'bingewatching',\n",
       " '#',\n",
       " 'nothingtodo',\n",
       " 'üòé']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk stands for Natural Language Toolkit\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66d7c758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'ll\", 'recently', 'watched', \"o'clock\", 'this', 'show', \"'s\", 'called', 'mindhunters', ':', ')', '.', 'i', 'totally', 'loved', 'it', 'üòç', '.', 'it', 'was', 'gr8', '<', '3', '!', '#', 'bingewatching', '#', 'nothingtodo', 'üòé']\n"
     ]
    }
   ],
   "source": [
    "word = word_tokenize(corpus.lower(),language='english')\n",
    "\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "238b6973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'LL\", 'RECENTLY', 'WATCHED', \"O'CLOCK\", 'THIS', 'SHOW', \"'S\", 'CALLED', 'MINDHUNTERS', ':', ')', '.', 'I', 'TOTALLY', 'LOVED', 'IT', 'üòç', '.', 'IT', 'WAS', 'GR8', '<', '3', '!', '#', 'BINGEWATCHING', '#', 'NOTHINGTODO', 'üòé']\n"
     ]
    }
   ],
   "source": [
    "word = word_tokenize(corpus.upper())\n",
    "\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a4636",
   "metadata": {},
   "source": [
    "### Note:- \n",
    "\n",
    "* nltk's word tokenizer  breaks on whitespaces as well as it also breaks punctuation words such as \"I'll\" into \"I\" and \"'ll\", show's\" into \"show\" and \"'s\". On the other hand it doesn't break \"o'clock\" and treats it as a separate token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc42e3c9",
   "metadata": {},
   "source": [
    "## 2) Word Punctuation Tokenizer\n",
    "\n",
    "* It seprate text into individual words and also seprates all punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c4a5c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'\",\n",
       " 'll',\n",
       " 'recently',\n",
       " 'watched',\n",
       " 'o',\n",
       " \"'\",\n",
       " 'clock',\n",
       " 'this',\n",
       " 'show',\n",
       " \"'\",\n",
       " 's',\n",
       " 'called',\n",
       " 'mindhunters',\n",
       " ':).',\n",
       " 'I',\n",
       " 'totally',\n",
       " 'loved',\n",
       " 'it',\n",
       " 'üòç.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'gr8',\n",
       " '<',\n",
       " '3',\n",
       " '!',\n",
       " '#',\n",
       " 'bingewatching',\n",
       " '#',\n",
       " 'nothingtodo',\n",
       " 'üòé']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0a80d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'\", 'll', 'recently', 'watched', 'o', \"'\", 'clock', 'this', 'show', \"'\", 's', 'called', 'mindhunters', ':).', 'I', 'totally', 'loved', 'it', 'üòç.', 'It', 'was', 'gr8', '<', '3', '!', '#', 'bingewatching', '#', 'nothingtodo', 'üòé']\n"
     ]
    }
   ],
   "source": [
    "word_punctuation = wordpunct_tokenize(corpus)\n",
    "\n",
    "print(word_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a9fd81",
   "metadata": {},
   "source": [
    "## 3) Sentence Tokenizer\n",
    "\n",
    "\n",
    "* It splits or seprate the text into sentences. It Seprates sentences by .(fullstop) and !(Exclamation mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc00ef5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'll recently watched o'clock this show's called mindhunters:).\",\n",
       " 'I totally loved it üòç.',\n",
       " 'It was gr8 <3!',\n",
       " '#bingewatching #nothingtodo üòé']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df94f40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'll recently watched o'clock this show's called mindhunters:).\", 'I totally loved it üòç.', 'It was gr8 <3!', '#bingewatching #nothingtodo üòé']\n"
     ]
    }
   ],
   "source": [
    "sentence = sent_tokenize(corpus)\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d05c5",
   "metadata": {},
   "source": [
    "## 4) Tweer Tokenizer \n",
    "\n",
    "* Word tokenizer or punctuation word tokenizer it will seprate text emojis like \"<3\" into '<' and '3' and \":)\" into ':' and ')' which is something that we don't want.\n",
    "\n",
    "\n",
    "* Emojis have their own significance in areas like sentiment analysis where a happy face and sad face can alone prove to be a really good predictor of the sentiment.\n",
    "\n",
    "\n",
    "* Similarly, the hashtags are broken into two tokens. A hashtag is used for searching specific topics or photos in social media apps such as Instagram and facebook.\n",
    "\n",
    "\n",
    "* Tweet tokenizer breakdown text into individual tokens except text emojis, #(hashtags) and (') apostrophe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c001e876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'll\",\n",
       " 'recently',\n",
       " 'watched',\n",
       " \"o'clock\",\n",
       " 'this',\n",
       " \"show's\",\n",
       " 'called',\n",
       " 'mindhunters',\n",
       " ':)',\n",
       " '.',\n",
       " 'I',\n",
       " 'totally',\n",
       " 'loved',\n",
       " 'it',\n",
       " 'üòç',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'gr8',\n",
       " '<3',\n",
       " '!',\n",
       " '#bingewatching',\n",
       " '#nothingtodo',\n",
       " 'üòé']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "TweetTokenizer().tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddf75293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'll\",\n",
       " 'recently',\n",
       " 'watched',\n",
       " \"o'clock\",\n",
       " 'this',\n",
       " \"show's\",\n",
       " 'called',\n",
       " 'mindhunters',\n",
       " ':)',\n",
       " '.',\n",
       " 'I',\n",
       " 'totally',\n",
       " 'loved',\n",
       " 'it',\n",
       " 'üòç',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'gr8',\n",
       " '<3',\n",
       " '!',\n",
       " '#bingewatching',\n",
       " '#nothingtodo',\n",
       " 'üòé']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c84998e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'll\", 'recently', 'watched', \"o'clock\", 'this', \"show's\", 'called', 'mindhunters', ':)', '.', 'I', 'totally', 'loved', 'it', 'üòç', '.', 'It', 'was', 'gr8', '<3', '!', '#bingewatching', '#nothingtodo', 'üòé']\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.tokenize(corpus)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843936a1",
   "metadata": {},
   "source": [
    "## 5) Regular Expression Tokenizer\n",
    "\n",
    "* Regex Tokenizer breakdown and output only that text which matches with regex pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9f9ad79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#bingewatching', '#nothingtodo']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "text = corpus\n",
    "\n",
    "pattern = \"#\\w+\"\n",
    "\n",
    "regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e3c81",
   "metadata": {},
   "source": [
    "## 6) Tree Bank Word Tokenizer\n",
    "\n",
    "\n",
    "* It breakdown text into tokens whether it is emoji, hastag or punctuation marks. \n",
    "\n",
    "\n",
    "* It will not breakdown fullstop(.)\n",
    "\n",
    "\n",
    "* It will breakdown fullstop when there is a space before fullstop or the last fullstop of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "357a5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1c4c8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'My Name is Sameer.live in #Nepal. Hel?lo . jd.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7600c3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'Name',\n",
       " 'is',\n",
       " 'Sameer.live',\n",
       " 'in',\n",
       " '#',\n",
       " 'Nepal.',\n",
       " 'Hel',\n",
       " '?',\n",
       " 'lo',\n",
       " '.',\n",
       " 'jd',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97b52366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'Name', 'is', 'Sameer.live', 'in', '#', 'Nepal.', 'Hel', '?', 'lo', '.', 'jd', '.']\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.tokenize(text)\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d87856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
