{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "722950b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shaun': 4, 'is': 1, 'looking': 3, 'for': 0, 'job': 2}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "v = CountVectorizer()\n",
    "v.fit([\"Shaun is looking for a job is\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dff861",
   "metadata": {},
   "source": [
    "**Number shown in the output are the indexes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5512599",
   "metadata": {},
   "source": [
    "**To see the functions of CountVectorizer press v.'tab' but mainly we use ngram_range function to give the size of n-gram. By deafult ngram_range is (1,1), it means uni-gram.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e63a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shaun': 9,\n",
       " 'is': 2,\n",
       " 'looking': 6,\n",
       " 'for': 0,\n",
       " 'job': 5,\n",
       " 'shaun is': 10,\n",
       " 'is looking': 3,\n",
       " 'looking for': 7,\n",
       " 'for job': 1,\n",
       " 'shaun is looking': 11,\n",
       " 'is looking for': 4,\n",
       " 'looking for job': 8}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning range to CountVectorizer, means take 1,2 or 3 .. word combinations to create feature\n",
    "\n",
    "v = CountVectorizer(ngram_range=(1, 3))  # (min_n, max_n) \n",
    "v.fit([\"Shaun is looking for a job\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d77b7",
   "metadata": {},
   "source": [
    "**Number shown in the output are the indexes. These index no. are shown becaue it is a vocublary of words.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f8091",
   "metadata": {},
   "source": [
    "**In output first five lines are the results of uni-gram, 6-9 lines are the results of bi-gram, 10-12 lines are the result of tri-gram.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16fb6e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'absolutely': 0,\n",
       " 'wonderful': 14,\n",
       " 'silky': 11,\n",
       " 'and': 3,\n",
       " 'easy': 8,\n",
       " 'comfortable': 7,\n",
       " 'absolutely wonderful': 1,\n",
       " 'wonderful silky': 15,\n",
       " 'silky and': 12,\n",
       " 'and easy': 5,\n",
       " 'easy and': 9,\n",
       " 'and comfortable': 4,\n",
       " 'absolutely wonderful silky': 2,\n",
       " 'wonderful silky and': 16,\n",
       " 'silky and easy': 13,\n",
       " 'and easy and': 6,\n",
       " 'easy and comfortable': 10}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assigning range to CountVectorizer \n",
    "\n",
    "v = CountVectorizer(ngram_range=(1, 3))\n",
    "v.fit([\"Absolutely wonderful - silky and easy and comfortable\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc74ee",
   "metadata": {},
   "source": [
    "**Special character is present in vocabulary (-) but it is not shown in output becuase counter vectorizer remove special character before making features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77349b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['absolutely',\n",
       " 'absolutely wonderful',\n",
       " 'absolutely wonderful silky',\n",
       " 'and',\n",
       " 'and comfortable',\n",
       " 'and easy',\n",
       " 'and easy and',\n",
       " 'comfortable',\n",
       " 'easy',\n",
       " 'easy and',\n",
       " 'easy and comfortable',\n",
       " 'silky',\n",
       " 'silky and',\n",
       " 'silky and easy',\n",
       " 'wonderful',\n",
       " 'wonderful silky',\n",
       " 'wonderful silky and']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column names after applying N-Grams\n",
    "\n",
    "v.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c7b8d",
   "metadata": {},
   "source": [
    "**Here Index no. are not seen in code becuase these are feature names.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6111d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking stopwords in vocabulary\n",
    "\n",
    "v.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf0fb7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Thor ate pizza\",\n",
    "    \"Loki is tall\",\n",
    "    \"Logi is eating pizza\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d56fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "347f44e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to remove stop words and punctuation marks from text and apply lemmatization on text.\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # importing english language vocabulary from spacy library\n",
    "\n",
    "def preprocess(text):\n",
    "    # Remove stop words and lemmatize the text\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        filtered_tokens.append(token.lemma_)\n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb76ce1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thor eat pizza'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"Thor ate pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2271c218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loki eat pizza'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"Loki is eating pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15b8f0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thor eat pizza', 'Loki tall', 'Logi eat pizza']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_processed = [preprocess(text) for text in corpus]\n",
    "corpus_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a82a4bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thor': 8,\n",
       " 'eat': 0,\n",
       " 'pizza': 6,\n",
       " 'thor eat': 9,\n",
       " 'eat pizza': 1,\n",
       " 'loki': 4,\n",
       " 'tall': 7,\n",
       " 'loki tall': 5,\n",
       " 'logi': 2,\n",
       " 'logi eat': 3}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create CountVectorizer with n-gram range from 1 to 2\n",
    "\n",
    "v = CountVectorizer(ngram_range=(1, 2))\n",
    "v.fit(corpus_processed)\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4518df2e",
   "metadata": {},
   "source": [
    "**This is the vocabulary of corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "022367cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0, 1, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now generate bag of n gram vector for few sampleÂ documents\n",
    "\n",
    "v.transform([\"Thor eat pizza\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a902b273",
   "metadata": {},
   "source": [
    "**This is the vector representation of first document of corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e9a9c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a document that has out of vocabulary (OOV) term and see how bag of ngram generates vector out of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2295b1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform([\"Hulk eat pizza\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad97bbdf",
   "metadata": {},
   "source": [
    "**Hulk is out of vocabulary (OOV) term, that's why it is unable to predict correct output and generate random numbers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f57c16e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd40b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document,\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38feb851",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2, 2)) # (2,2) all features are the combinations of 2 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05cd621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "155019d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
       "       'second document', 'the first', 'the second', 'the third',\n",
       "       'third one', 'this document', 'this is', 'this the'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde1c1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
